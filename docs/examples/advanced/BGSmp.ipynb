{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ".. _BGS_multiprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced example: background selection (revisited)\n",
    "\n",
    "This is the same background selection simulation as in the previous example, but with the following change to the implementation details:\n",
    "\n",
    "* We change the nature of the parallelism.  The previous example uses fwdpy to run 40 simulations at a time, process them, and then repeat the process 25 times, doing all of the analysis in-memory.  Here, we use the multiprocessing module to spawn 40 separate Python processes.  Each process runs 25 simulations and records the summary statistics.  At the end of the 25 replicates, the data are written to an SQLite database and get the mean values via an SQL query, which is out-of-memory.\n",
    "\n",
    "The purpose of this example is to show that there are multiple ways to do things in terms of how to use parallel processing to perform simulations.  Further, the technique of writing results to an SQLite database is very powerful as it allows many analyses (\"aggregations\") to be done without loading all of your simulation results into RAM.\n",
    "\n",
    "## Rationale\n",
    "\n",
    "We designed fwdpy to run simulations in parallel with ease.  The design enables work flows where you run a \"batch\" of simulations and then process them as you need.  However, this can create a bottleneck on the CPU resources when the processing is done back on the Python side:\n",
    "\n",
    "1. Run a bunch of simulations in parallel\n",
    "2. Process each replicate serially\n",
    "3. Return to step 1 until you've obtained the desired number of replicates\n",
    "\n",
    "An alternative is to use Python's built-in support for multiprocessing, which boils down to running jobs in separate Python threads and thus bypassing the dreaded GIL.  This design allows you to set up the equivalent of a thread pool for running your simulations. Each process runs a single simulation, processes it, and then exits.\n",
    "\n",
    "The main challenge with this design is that it requires better organization on your end. You need to write the function that will be farmed out to other processes, and its arguments must consist of pure Python types, meaning that none of fwdpy's Cython-based classes can be used.\n",
    "\n",
    "The other challenge is synchronizing the output.  Here, we use multiprocessing.Lock to do that for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Use Python 3's print a a function.\n",
    "#This future-proofs the code in the notebook\n",
    "from __future__ import print_function\n",
    "#Import fwdpy.  Give it a shorter name\n",
    "import fwdpy as fp\n",
    "##Other libs we need\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import sqlite3\n",
    "import multiprocessing as mp\n",
    "import libsequence.polytable as polyt\n",
    "import libsequence.summstats as sstats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the function that we will run in separate Python processes\n",
    "\n",
    "The details of setting up the simulation are identical to the prevous BGS example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LOCK=mp.Lock()\n",
    "def simulate_async(args):\n",
    "    \"\"\"\n",
    "    This function will be run in a separate process\n",
    "    using the multiprocessing module.  Its argument \n",
    "    list is a tuple.\n",
    "\n",
    "    \"\"\"\n",
    "    #Assign names to the tuple values\n",
    "    seed,dbname,tablename = args\n",
    "    \n",
    "    # Where neutral mutations occur:\n",
    "    nregions = [fp.Region(beg=0,end=1,weight=1)]\n",
    "\n",
    "    # Where selected mutations occur:\n",
    "    sregions = [fp.ConstantS(beg=-1,end=0,weight=1,s=-0.05,h=1),\n",
    "                fp.ConstantS(beg=1,end=2,weight=1,s=-0.05,h=1)]\n",
    "\n",
    "    # Recombination:\n",
    "    recregions = [fp.Region(beg=-1,end=2,weight=1)]\n",
    "\n",
    "    #Population size\n",
    "    N=1000\n",
    "    #We'll evolve for 10N generations.\n",
    "    #nlist is a list of population sizes over time.\n",
    "    #len(nlist) is the length of the simulation\n",
    "    #We use numpy arrays for speed and optimised RAM\n",
    "    #use.  Note the dtype=np.uint32, which means 32-bit\n",
    "    #unsigned integer. Failure to use this type will\n",
    "    #cause a run-time error.\n",
    "    nlist = np.array([N]*(10*N),dtype=np.uint32)\n",
    "\n",
    "    #Initalize a random number generator with seed value of 101\n",
    "    rng = fp.GSLrng(seed)\n",
    "\n",
    "    pops = fp.evolve_regions(rng,  \n",
    "                         1,       #Simulate only 1 population at a time     \n",
    "                         N,        \n",
    "                         nlist[0:],\n",
    "                         0.005,    \n",
    "                         0.01,     \n",
    "                         0.005,    \n",
    "                         nregions, \n",
    "                         sregions, \n",
    "                         recregions)\n",
    "    sample = fp.get_samples(rng,pops[0],20)\n",
    "    simdatasNeut = polyt.SimData(sample[0])\n",
    "    polySIMn = sstats.PolySIM(simdatasNeut)\n",
    "    ##Append stats into our growing DataFrame:\n",
    "    summstats=[{'thetapi':polySIMn.thetapi(),'npoly':polySIMn.numpoly(),'thetaw':polySIMn.thetaw()}]\n",
    "    DF=pd.DataFrame(summstats)\n",
    "    \n",
    "    #We must prevent multiple processes from\n",
    "    #writing to the database at once.\n",
    "    #We use our global lock as a mutex \n",
    "    #to ensure that only 1 process is writing\n",
    "    #at a time.\n",
    "    LOCK.acquire()\n",
    "    con = sqlite3.connect(dbname)\n",
    "    DF.to_sql(tablename,con,if_exists='append',index=False)\n",
    "    con.close()\n",
    "    LOCK.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the simulations\n",
    "\n",
    "The following block of code sets up a thread pool to run the above function using 40 separate processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if os.path.isfile('BGSmp.db'):\n",
    "    os.remove('BGSmp.db')\n",
    "np.random.seed(101)\n",
    "#Generate the arguments to pass to simulate_async.\n",
    "#The arguments for mp.Pool.imap_unordered must be a tuple.\n",
    "#Our list of arguments will be 1000 elements long. Each tuple\n",
    "#contains a random seed.  If this were a study for publication,\n",
    "#I would be more careful and guarantee that each seed is unique.\n",
    "args=[(seed,'BGSmp.db','stats') for seed in np.random.randint(0,42000000,1000)]\n",
    "#P a thread pool using the number of processors on your machine\n",
    "#If you have < 40 cores, it'll spawn new processes as old ones finish.\n",
    "P=mp.Pool() \n",
    "#Pass the arguments along to the process pool.\n",
    "#This will run 1,000 replicate simulations\n",
    "#and output data from each to our sqlite3 \n",
    "#database.\n",
    "P.imap_unordered(simulate_async,args)\n",
    "P.close()\n",
    "P.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the mean diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   avg(npoly)  avg(thetapi)  avg(thetaw)\n",
      "0      57.635     16.033353    16.245555\n"
     ]
    }
   ],
   "source": [
    "#open database connection:\n",
    "c=sqlite3.connect('BGSmp.db')\n",
    "#Get means for each column:\n",
    "x=pd.read_sql_query('select avg(npoly),avg(thetapi),avg(thetaw) from stats',c)\n",
    "c.close()\n",
    "os.remove('BGSmp.db')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'thetapi' record is our mean $\\pi$ from all of the simulations, and it is quite close to the theoretical value. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
